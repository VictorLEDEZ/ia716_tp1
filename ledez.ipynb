{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>\n",
    "\n",
    "# **TP CycleGAN : Victor LEDEZ**\n",
    "\n",
    "<hr/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Concernant le projet 1 je vous demande d'entrainer un GAN qui va apprendre a transformer les images de MNIST en image de SVHN. Une fois que vous avez réussi à faire cela je vous laisse vous débrouiller pour réussir à entrainer un CNN pour qu'il puisse marché sur les images du set de test de SVHN tout en ayant aucune annotations pour entrainer sur SVHN. Vous pouvez considérer que vous avez les images de MNIST avec les annotations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Imports**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Configs and Utils**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TRAIN_DIR = \"data/train\"\n",
    "VAL_DIR = \"data/val\"\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 1e-5\n",
    "LAMBDA_IDENTITY = 0.0\n",
    "LAMBDA_CYCLE = 10\n",
    "NUM_WORKERS = 4\n",
    "NUM_EPOCHS = 10\n",
    "LOAD_MODEL = True\n",
    "SAVE_MODEL = True\n",
    "CHECKPOINT_GEN_MNIST = \"genMNIST.pth.tar\"\n",
    "CHECKPOINT_GEN_SVHN = \"genSVHN.pth.tar\"\n",
    "CHECKPOINT_CRITIC_MNIST = \"criticMNIST.pth.tar\"\n",
    "CHECKPOINT_CRITIC_SVHN = \"criticSVHN.pth.tar\"\n",
    "\n",
    "BATCH_SIZE_TRAIN = 64\n",
    "BATCH_SIZE_TEST = 1000\n",
    "IMAGE_SIZE = 32\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(model, optimizer, filename=\"my_checkpoint.pth.tar\"):\n",
    "    print(\"=> Saving checkpoint\")\n",
    "    checkpoint = {\n",
    "        \"state_dict\": model.state_dict(),\n",
    "        \"optimizer\": optimizer.state_dict(),\n",
    "    }\n",
    "    torch.save(checkpoint, filename)\n",
    "\n",
    "\n",
    "def load_checkpoint(checkpoint_file, model, optimizer, lr):\n",
    "    print(\"=> Loading checkpoint\")\n",
    "    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # If we don't do this then it will just have learning rate of old checkpoint\n",
    "    # and it will lead to many hours of debugging \\:\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group[\"lr\"] = lr\n",
    "\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Loading Data**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loader():\n",
    "    \"\"\"Builds and returns Dataloader for MNIST and SVHN dataset.\"\"\"\n",
    "\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Scale(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "    svhn = datasets.SVHN(root='./svhn',\n",
    "                         download=True, transform=transform)\n",
    "    mnist = datasets.MNIST(root='./mnist',\n",
    "                           download=True, transform=transform)\n",
    "\n",
    "    svhn_loader = torch.utils.data.DataLoader(dataset=svhn,\n",
    "                                              batch_size=BATCH_SIZE,\n",
    "                                              shuffle=True,\n",
    "                                              num_workers=0)\n",
    "\n",
    "    mnist_loader = torch.utils.data.DataLoader(dataset=mnist,\n",
    "                                               batch_size=BATCH_SIZE,\n",
    "                                               shuffle=True,\n",
    "                                               num_workers=0)\n",
    "    return svhn_loader, mnist_loader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Discriminator Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 4, stride,\n",
    "                      1, bias=True, padding_mode='reflect'),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels=3, features=[64, 128, 256, 512]):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(\n",
    "                in_channels,\n",
    "                features[0],\n",
    "                kernel_size=4,\n",
    "                stride=2,\n",
    "                padding=1,\n",
    "                padding_mode='reflect'\n",
    "            ),\n",
    "            nn.LeakyReLU(0.2)\n",
    "        )\n",
    "\n",
    "        layers = []\n",
    "        in_channels = features[0]\n",
    "        for feature in features[1:]:\n",
    "            layers.append(Block(in_channels, feature,\n",
    "                          stride=1 if feature == features[-1] else 2))\n",
    "            in_channels = feature\n",
    "        layers.append(nn.Conv2d(in_channels, 1, kernel_size=4,\n",
    "                      stride=1, padding=1, padding_mode='reflect'))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        return torch.sigmoid(self.model(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Generator Model**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels,\n",
    "                      padding_mode='reflect', **kwargs)\n",
    "            if down\n",
    "            else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n",
    "            nn.InstanceNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True) if use_act else nn.Identity()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.block = nn.Sequential(\n",
    "            ConvBlock(channels, channels, kernel_size=3, padding=1),\n",
    "            ConvBlock(channels, channels, use_act=False,\n",
    "                      kernel_size=3, padding=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x + self.block(x)\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, img_channels, num_features=64, num_residuals=9):\n",
    "        super().__init__()\n",
    "\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(img_channels, num_features, kernel_size=7,\n",
    "                      stride=1, padding=3, padding_mode='reflect'),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.down_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(num_features, num_features*2,\n",
    "                          kernel_size=3, stride=2, padding=1),\n",
    "                ConvBlock(num_features*2, num_features*4,\n",
    "                          kernel_size=3, stride=2, padding=1)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.residual_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(num_features*4) for _ in range(num_residuals)]\n",
    "        )\n",
    "\n",
    "        self.up_blocks = nn.ModuleList(\n",
    "            [\n",
    "                ConvBlock(num_features*4, num_features*2, down=False,\n",
    "                          kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                ConvBlock(num_features*2, num_features, down=False,\n",
    "                          kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.last = nn.Conv2d(num_features*1, img_channels, kernel_size=7,\n",
    "                              stride=1, padding=3, padding_mode='reflect')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        for layer in self.down_blocks:\n",
    "            x = layer(x)\n",
    "        x = self.residual_blocks(x)\n",
    "        for layer in self.up_blocks:\n",
    "            x = layer(x)\n",
    "        return torch.tanh(self.last(x))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Train**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ledez\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torchvision\\transforms\\transforms.py:317: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
      "  warnings.warn(\"The use of the transforms.Scale transform is deprecated, \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to ./svhn\\train_32x32.mat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "182041600it [00:31, 5855440.67it/s]                                \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./mnist\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9913344it [00:00, 15718795.68it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./mnist\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29696it [00:00, 29712321.47it/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./mnist\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1649664it [00:00, 16797743.97it/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./mnist\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./mnist\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5120it [00:00, 5113056.30it/s]          \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./mnist\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./mnist\\MNIST\\raw\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ledez\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\cuda\\amp\\grad_scaler.py:115: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\"torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\")\n",
      "  0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14232/4239819953.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 138\u001b[1;33m     train_fn(disc_MNIST, disc_SVHN, gen_SVHN, gen_MNIST, loader,\n\u001b[0m\u001b[0;32m    139\u001b[0m              opt_disc, opt_gen, L1, mse, d_scaler, g_scaler)\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_14232/4239819953.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[1;34m(disc_MNIST, disc_SVHN, gen_SVHN, gen_MNIST, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mloop\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mSVHN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[0mSVHN\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSVHN\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mMNIST\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mMNIST\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "def train_fn(disc_MNIST, disc_SVHN, gen_SVHN, gen_MNIST, loader, opt_disc, opt_gen, l1, mse, d_scaler, g_scaler):\n",
    "    MNIST_reals = 0\n",
    "    MNIST_fakes = 0\n",
    "    loop = tqdm(loader, leave=True)\n",
    "\n",
    "    for idx, (SVHN, MNIST) in enumerate(loop):\n",
    "        SVHN = SVHN.to(DEVICE)\n",
    "        MNIST = MNIST.to(DEVICE)\n",
    "\n",
    "        # Train Discriminators MNIST and SVHN\n",
    "        with torch.cuda.amp.autocast():\n",
    "            fake_MNIST = gen_MNIST(SVHN)\n",
    "            D_MNIST_real = disc_MNIST(MNIST)\n",
    "            D_MNIST_fake = disc_MNIST(fake_MNIST.detach())\n",
    "            MNIST_reals += D_MNIST_real.mean().item()\n",
    "            MNIST_fakes += D_MNIST_fake.mean().item()\n",
    "            D_MNIST_real_loss = mse(\n",
    "                D_MNIST_real, torch.ones_like(D_MNIST_real))\n",
    "            D_MNIST_fake_loss = mse(\n",
    "                D_MNIST_fake, torch.zeros_like(D_MNIST_fake))\n",
    "            D_MNIST_loss = D_MNIST_real_loss + D_MNIST_fake_loss\n",
    "\n",
    "            fake_SVHN = gen_SVHN(MNIST)\n",
    "            D_SVHN_real = disc_SVHN(SVHN)\n",
    "            D_SVHN_fake = disc_SVHN(fake_SVHN.detach())\n",
    "            D_SVHN_real_loss = mse(D_SVHN_real, torch.ones_like(D_SVHN_real))\n",
    "            D_SVHN_fake_loss = mse(D_SVHN_fake, torch.zeros_like(D_SVHN_fake))\n",
    "            D_SVHN_loss = D_SVHN_real_loss + D_SVHN_fake_loss\n",
    "\n",
    "            # put it together\n",
    "            D_loss = (D_MNIST_loss + D_SVHN_loss)/2\n",
    "\n",
    "        opt_disc.zero_grad()\n",
    "        d_scaler.scale(D_loss).backward()\n",
    "        d_scaler.step(opt_disc)\n",
    "        d_scaler.update()\n",
    "\n",
    "        # Train Generators MNIST and SVHN\n",
    "        with torch.cuda.amp.autocast():\n",
    "            # adversarial loss for both generators\n",
    "            D_MNIST_fake = disc_MNIST(fake_MNIST)\n",
    "            D_SVHN_fake = disc_SVHN(fake_SVHN)\n",
    "            loss_G_MNIST = mse(D_MNIST_fake, torch.ones_like(D_MNIST_fake))\n",
    "            loss_G_SVHN = mse(D_SVHN_fake, torch.ones_like(D_SVHN_fake))\n",
    "\n",
    "            # cycle loss\n",
    "            cycle_SVHN = gen_SVHN(fake_MNIST)\n",
    "            cycle_MNIST = gen_MNIST(fake_SVHN)\n",
    "            cycle_SVHN_loss = l1(SVHN, cycle_SVHN)\n",
    "            cycle_MNIST_loss = l1(MNIST, cycle_MNIST)\n",
    "\n",
    "            # identity loss (remove these for efficiency if you set lambda_identity=0)\n",
    "            identity_SVHN = gen_SVHN(SVHN)\n",
    "            identity_MNIST = gen_MNIST(MNIST)\n",
    "            identity_SVHN_loss = l1(SVHN, identity_SVHN)\n",
    "            identity_MNIST_loss = l1(MNIST, identity_MNIST)\n",
    "\n",
    "            # add all together\n",
    "            G_loss = (\n",
    "                loss_G_SVHN\n",
    "                + loss_G_MNIST\n",
    "                + cycle_SVHN_loss * LAMBDA_CYCLE\n",
    "                + cycle_MNIST_loss * LAMBDA_CYCLE\n",
    "                + identity_MNIST_loss * LAMBDA_IDENTITY\n",
    "                + identity_SVHN_loss * LAMBDA_IDENTITY\n",
    "            )\n",
    "\n",
    "        opt_gen.zero_grad()\n",
    "        g_scaler.scale(G_loss).backward()\n",
    "        g_scaler.step(opt_gen)\n",
    "        g_scaler.update()\n",
    "\n",
    "        if idx % 200 == 0:\n",
    "            save_image(fake_MNIST*0.5+0.5, f\"saved_images/MNIST_{idx}.png\")\n",
    "            save_image(fake_SVHN*0.5+0.5, f\"saved_images/SVHN_{idx}.png\")\n",
    "\n",
    "        loop.set_postfix(MNIST_real=MNIST_reals/(idx+1),\n",
    "                         MNIST_fake=MNIST_fakes/(idx+1))\n",
    "\n",
    "\n",
    "disc_MNIST = Discriminator(in_channels=3).to(DEVICE)\n",
    "disc_SVHN = Discriminator(in_channels=3).to(DEVICE)\n",
    "gen_SVHN = Generator(img_channels=3, num_residuals=9).to(DEVICE)\n",
    "gen_MNIST = Generator(img_channels=3, num_residuals=9).to(DEVICE)\n",
    "opt_disc = optim.Adam(\n",
    "    list(disc_MNIST.parameters()) + list(disc_SVHN.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "\n",
    "opt_gen = optim.Adam(\n",
    "    list(gen_SVHN.parameters()) + list(gen_MNIST.parameters()),\n",
    "    lr=LEARNING_RATE,\n",
    "    betas=(0.5, 0.999),\n",
    ")\n",
    "\n",
    "L1 = nn.L1Loss()\n",
    "mse = nn.MSELoss()\n",
    "\n",
    "# if LOAD_MODEL:\n",
    "#     load_checkpoint(\n",
    "#         CHECKPOINT_GEN_MNIST, gen_MNIST, opt_gen, LEARNING_RATE,\n",
    "#     )\n",
    "#     load_checkpoint(\n",
    "#         CHECKPOINT_GEN_SVHN, gen_SVHN, opt_gen, LEARNING_RATE,\n",
    "#     )\n",
    "#     load_checkpoint(\n",
    "#         CHECKPOINT_CRITIC_MNIST, disc_MNIST, opt_disc, LEARNING_RATE,\n",
    "#     )\n",
    "#     load_checkpoint(\n",
    "#         CHECKPOINT_CRITIC_SVHN, disc_SVHN, opt_disc, LEARNING_RATE,\n",
    "#     )\n",
    "\n",
    "# dataset = MnistSvhnDataset(\n",
    "#     root_MNIST=TRAIN_DIR+\"/MNISTs\", root_SVHN=TRAIN_DIR+\"/SVHNs\", transform=transforms\n",
    "# )\n",
    "# val_dataset = MnistSvhnDataset(\n",
    "#     root_MNIST=\"cyclegan_test/MNIST1\", root_SVHN=\"cyclegan_test/SVHN1\", transform=transforms\n",
    "# )\n",
    "# val_loader = DataLoader(\n",
    "#     val_dataset,\n",
    "#     batch_size=1,\n",
    "#     shuffle=False,\n",
    "#     pin_memory=True,\n",
    "# )\n",
    "# loader = DataLoader(\n",
    "#     dataset,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     shuffle=True,\n",
    "#     num_workers=NUM_WORKERS,\n",
    "#     pin_memory=True\n",
    "# )\n",
    "loader = get_loader()\n",
    "g_scaler = torch.cuda.amp.GradScaler()\n",
    "d_scaler = torch.cuda.amp.GradScaler()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_fn(disc_MNIST, disc_SVHN, gen_SVHN, gen_MNIST, loader,\n",
    "             opt_disc, opt_gen, L1, mse, d_scaler, g_scaler)\n",
    "\n",
    "    # if SAVE_MODEL:\n",
    "    #     save_checkpoint(gen_MNIST, opt_gen, filename=CHECKPOINT_GEN_MNIST)\n",
    "    #     save_checkpoint(gen_SVHN, opt_gen, filename=CHECKPOINT_GEN_SVHN)\n",
    "    #     save_checkpoint(disc_MNIST, opt_disc,\n",
    "    #                     filename=CHECKPOINT_CRITIC_MNIST)\n",
    "    #     save_checkpoint(disc_SVHN, opt_disc,\n",
    "    #                     filename=CHECKPOINT_CRITIC_SVHN)\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f489fa09b09661d741d1aee0482c3792118d516f8dffab1399d2c4234cb3cfaa"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
